{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T16:36:59.666941Z","iopub.status.busy":"2024-09-27T16:36:59.666557Z","iopub.status.idle":"2024-09-27T16:37:00.066619Z","shell.execute_reply":"2024-09-27T16:37:00.065119Z","shell.execute_reply.started":"2024-09-27T16:36:59.666903Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","df=pd.read_csv('/kaggle/input/social/Social.csv')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T16:37:22.604182Z","iopub.status.busy":"2024-09-27T16:37:22.603783Z","iopub.status.idle":"2024-09-27T16:37:26.800174Z","shell.execute_reply":"2024-09-27T16:37:26.799255Z","shell.execute_reply.started":"2024-09-27T16:37:22.604140Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from transformers import BertTokenizer, BertModel\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T16:37:26.802174Z","iopub.status.busy":"2024-09-27T16:37:26.801707Z","iopub.status.idle":"2024-09-27T16:37:26.806789Z","shell.execute_reply":"2024-09-27T16:37:26.805823Z","shell.execute_reply.started":"2024-09-27T16:37:26.802138Z"},"trusted":true},"outputs":[],"source":["# Transformations for Image Preprocessing\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T16:37:38.128796Z","iopub.status.busy":"2024-09-27T16:37:38.128389Z","iopub.status.idle":"2024-09-27T16:37:39.790380Z","shell.execute_reply":"2024-09-27T16:37:39.789375Z","shell.execute_reply.started":"2024-09-27T16:37:38.128757Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4089ae40efe04af5b58476c104999f89","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38cd909e14c84bf0985ee58f5f2fe97f","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60bb37bbe968485d97344b93aca19292","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae7a4c9d23964e8886257740b98f45f7","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Data Preparation\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T16:37:33.067568Z","iopub.status.busy":"2024-09-27T16:37:33.067147Z","iopub.status.idle":"2024-09-27T16:37:33.072068Z","shell.execute_reply":"2024-09-27T16:37:33.071163Z","shell.execute_reply.started":"2024-09-27T16:37:33.067531Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from transformers import BertTokenizer, BertModel\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","\n","# VGG16 with Autoencoder for Image Feature Extraction\n","class ImageEncoder(nn.Module):\n","    def __init__(self):\n","        super(ImageEncoder, self).__init__()\n","        vgg16 = models.vgg16(pretrained=True)\n","        self.vgg16_features = vgg16.features\n","        \n","        # Encoder part of Autoencoder\n","        self.encoder = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 1024),  # Assuming VGG output size (512, 7, 7)\n","            nn.ReLU(),\n","            nn.Linear(1024, 256)  # Latent space representation\n","        )\n","        \n","        # Decoder part of Autoencoder\n","        self.decoder = nn.Sequential(\n","            nn.Linear(256, 1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, 512 * 7 * 7),\n","            nn.Sigmoid()  # Reconstruct the input (if needed during training)\n","        )\n","    \n","    def forward(self, x):\n","        # Extract features from VGG16\n","        with torch.no_grad():\n","            x = self.vgg16_features(x)\n","        x = torch.flatten(x, start_dim=1)\n","        \n","        # Encoder to get latent representation\n","        latent_rep = self.encoder(x)\n","        \n","        # Decoder for training (optional, only needed if you want to reconstruct input during training)\n","        reconstructed = self.decoder(latent_rep)\n","        \n","        return latent_rep, reconstructed\n","\n","# Text Encoder (e.g., BERT for Tweets)\n","class TextEncoder(nn.Module):\n","    def __init__(self):\n","        super(TextEncoder, self).__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","    \n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        return outputs.pooler_output  # CLS token embedding\n","\n","# Transformer Encoder for Combined Features\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, input_size, num_heads=4, ff_size=512, num_layers=2):\n","        super(TransformerEncoder, self).__init__() \n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=input_size, nhead=num_heads, dim_feedforward=ff_size\n","        )\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","\n","    def forward(self, x):\n","        x = self.transformer_encoder(x)\n","        return x\n","\n","# Bot Classification Model\n","class BotDetectionModel(nn.Module):\n","    def __init__(self):\n","        super(BotDetectionModel, self).__init__()\n","        self.image_encoder = ImageEncoder()\n","        self.text_encoder = TextEncoder()\n","        self.transformer_encoder = TransformerEncoder(input_size=256 + 768)  # Image features (256) + Text embeddings (768)\n","        self.dense_layer = nn.Sequential(\n","            nn.Linear(256 + 768 + 2, 128),  # Adding followers and following counts\n","            nn.ReLU(),\n","            nn.Linear(128, 1),  # Binary classification (Bot or Not)\n","            nn.Sigmoid()\n","        )\n","    \n","    def forward(self, images, input_ids, attention_mask, followers_count, following_count):\n","        # Process image inputs through VGG16 + Autoencoder (latent representation only)\n","        image_features, _ = self.image_encoder(images)\n","        \n","        # Process text inputs through BERT\n","        text_embeddings = self.text_encoder(input_ids, attention_mask)\n","        \n","        # Concatenate image and text embeddings\n","        combined_features = torch.cat((image_features, text_embeddings), dim=1)\n","        \n","        # Forward through Transformer Encoder\n","        transformer_output = self.transformer_encoder(combined_features.unsqueeze(1))  # Add sequence dimension\n","        \n","        # Add follower and following counts\n","        combined_with_counts = torch.cat((transformer_output.squeeze(1), followers_count, following_count), dim=1)\n","        \n","        # Dense layer for final classification\n","        output = self.dense_layer(combined_with_counts)\n","        return output\n","\n","# Custom Dataset Class for Bot Detection\n","class BotDetectionDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, transform=None):\n","        self.dataframe = dataframe\n","        self.tokenizer = tokenizer\n","        self.transform = transform\n","    \n","    def load_image_from_url(self, url):\n","        try:\n","            response = requests.get(url)\n","            img = Image.open(BytesIO(response.content)).convert('RGB')\n","            if self.transform:\n","                img = self.transform(img)\n","            return img\n","        except:\n","            return torch.zeros(3, 224, 224)  # Handle invalid or missing images\n","    \n","    def __len__(self):\n","        return len(self.dataframe)\n","    \n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","\n","        # Load and preprocess images\n","        profile_image = self.load_image_from_url(row['profile_image_url'])\n","        banner_image = self.load_image_from_url(row['profile_banner_url'])\n","        post_images = torch.stack([self.load_image_from_url(url) for url in eval(row['posts_url'])], dim=0).mean(0)  # Average across post images\n","        \n","        images = torch.stack([profile_image, banner_image, post_images], dim=0).mean(0)  # Average profile, banner, and post images\n","        \n","        # Tokenize the tweet text\n","        tweet_text = row['Tweet']\n","        encoding = self.tokenizer(tweet_text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n","        input_ids = encoding['input_ids'].squeeze(0)\n","        attention_mask = encoding['attention_mask'].squeeze(0)\n","        \n","        # Followers and following counts\n","        followers_count = torch.tensor([row['followers_count']], dtype=torch.float32)\n","        following_count = torch.tensor([row['friends_count']], dtype=torch.float32)\n","\n","        # Label (0: Human, 1: Bot)\n","        label = torch.tensor(1 if row['result'] == 'bot' else 0, dtype=torch.float32)\n","        \n","        return images, input_ids, attention_mask, followers_count, following_count, label\n","\n","# Transformations for Image Preprocessing\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])\n","\n","# Data Preparation\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Assume 'df' is the pandas DataFrame contai\n","# Creating Dataset and DataLoader\n","dataset = BotDetectionDataset(df, tokenizer, transform=transform)\n","dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n","\n","# Training the Model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = BotDetectionModel().to(device)\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T18:03:52.925651Z","iopub.status.busy":"2024-09-27T18:03:52.925223Z","iopub.status.idle":"2024-09-27T18:30:42.537882Z","shell.execute_reply":"2024-09-27T18:30:42.536832Z","shell.execute_reply.started":"2024-09-27T18:03:52.925610Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Loss: 1.0211989841892481\n","Epoch [2/10], Loss: 0.8612674545519841\n","Epoch [3/10], Loss: 1.3297130643904949\n","Epoch [4/10], Loss: 1.158986344436033\n","Epoch [5/10], Loss: 0.5306251735530608\n","Epoch [6/10], Loss: 0.4608929750000876\n","Epoch [7/10], Loss: 0.3460945374338153\n","Epoch [8/10], Loss: 1.06889152570109\n","Epoch [9/10], Loss: 0.9070639966773585\n"]}],"source":["# Training Loop\n","for epoch in range(9):\n","    model.train()\n","    running_loss = 0.0\n","    for images, input_ids, attention_mask, followers_count, following_count, labels in dataloader:\n","        images = images.to(device)\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        followers_count = followers_count.to(device)\n","        following_count = following_count.to(device)\n","        labels = labels.to(device)\n","        \n","        optimizer.zero_grad()\n","        \n","        outputs = model(images, input_ids, attention_mask, followers_count, following_count)\n","        loss = criterion(outputs.squeeze(), labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    \n","    print(f\"Epoch [{epoch+1}/10], Loss: {running_loss/len(dataloader)}\")"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T18:40:48.526038Z","iopub.status.busy":"2024-09-27T18:40:48.525059Z","iopub.status.idle":"2024-09-27T18:40:48.972820Z","shell.execute_reply":"2024-09-27T18:40:48.971808Z","shell.execute_reply.started":"2024-09-27T18:40:48.525998Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Bot Probability: 0.0064884936437010765\n"]}],"source":["# Inference on Specific Account\n","model.eval()\n","with torch.no_grad():\n","    # Sample account info (as you would provide during inference)\n","    sample_row = df.iloc[0]\n","    images, input_ids, attention_mask, followers_count, following_count, _ = dataset[0]\n","    \n","    # Add batch dimension and move to device\n","    images = images.unsqueeze(0).to(device)\n","    input_ids = input_ids.unsqueeze(0).to(device)\n","    attention_mask = attention_mask.unsqueeze(0).to(device)\n","    followers_count = followers_count.unsqueeze(0).to(device)\n","    following_count = following_count.unsqueeze(0).to(device)\n","    \n","    # Model prediction\n","    prediction = model(images, input_ids, attention_mask, followers_count, following_count)\n","    print(f\"Bot Probability: {prediction.item()}\")"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T18:01:33.810420Z","iopub.status.busy":"2024-09-27T18:01:33.810017Z","iopub.status.idle":"2024-09-27T18:01:34.037600Z","shell.execute_reply":"2024-09-27T18:01:34.036555Z","shell.execute_reply.started":"2024-09-27T18:01:33.810384Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["id                                                            468385317\n","created_at_year                                                    2019\n","screen_name                                                MariaNicola2\n","profile_image_url     http://pbs.twimg.com/profile_images/7956854877...\n","profile_banner_url    https://pbs.twimg.com/profile_banners/46838531...\n","followers_count                                                     287\n","friends_count                                                       165\n","result                                                              bot\n","posts_url             ['https://pbs.twimg.com/media/Ecjk1g8WAAIhxrV?...\n","Tweet                 Only enough yet popular determine internationa...\n","Name: 500, dtype: object\n","Bot Probability: 0.9998499155044556\n"]}],"source":["model.eval()\n","with torch.no_grad():\n","    # Sample account info (as you would provide during inference)\n","    sample_row = df.iloc[500]\n","    print(sample_row)\n","    images, input_ids, attention_mask, followers_count, following_count, _ = dataset[500]\n","    \n","    # Add batch dimension and move to device\n","    images = images.unsqueeze(0).to(device)\n","    input_ids = input_ids.unsqueeze(0).to(device)\n","    attention_mask = attention_mask.unsqueeze(0).to(device)\n","    followers_count = followers_count.unsqueeze(0).to(device)\n","    following_count = following_count.unsqueeze(0).to(device)\n","    \n","    # Model prediction\n","    prediction = model(images, input_ids, attention_mask, followers_count, following_count)\n","    print(f\"Bot Probability: {prediction.item()}\")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T06:26:24.963236Z","iopub.status.busy":"2024-09-24T06:26:24.962612Z","iopub.status.idle":"2024-09-24T06:26:28.263805Z","shell.execute_reply":"2024-09-24T06:26:28.262967Z","shell.execute_reply.started":"2024-09-24T06:26:24.963194Z"},"trusted":true},"outputs":[],"source":["# Save the model\n","torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","}, 'bot_detection_model.pth')\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T19:19:57.122023Z","iopub.status.busy":"2024-09-24T19:19:57.121249Z","iopub.status.idle":"2024-09-24T19:19:57.150687Z","shell.execute_reply":"2024-09-24T19:19:57.149820Z","shell.execute_reply.started":"2024-09-24T19:19:57.121978Z"},"trusted":true},"outputs":[{"data":{"text/plain":["BotDetectionModel(\n","  (image_encoder): ImageEncoder(\n","    (vgg16_features): Sequential(\n","      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (6): ReLU(inplace=True)\n","      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (8): ReLU(inplace=True)\n","      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (11): ReLU(inplace=True)\n","      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (13): ReLU(inplace=True)\n","      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (15): ReLU(inplace=True)\n","      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (18): ReLU(inplace=True)\n","      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (20): ReLU(inplace=True)\n","      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (22): ReLU(inplace=True)\n","      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (25): ReLU(inplace=True)\n","      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (27): ReLU(inplace=True)\n","      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (29): ReLU(inplace=True)\n","      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (encoder): Sequential(\n","      (0): Linear(in_features=25088, out_features=1024, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=1024, out_features=256, bias=True)\n","    )\n","    (decoder): Sequential(\n","      (0): Linear(in_features=256, out_features=1024, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=1024, out_features=25088, bias=True)\n","      (3): Sigmoid()\n","    )\n","  )\n","  (text_encoder): TextEncoder(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSdpaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","  )\n","  (transformer_encoder): TransformerEncoder(\n","    (transformer_encoder): TransformerEncoder(\n","      (layers): ModuleList(\n","        (0-1): 2 x TransformerEncoderLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (linear1): Linear(in_features=1024, out_features=512, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (linear2): Linear(in_features=512, out_features=1024, bias=True)\n","          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout1): Dropout(p=0.1, inplace=False)\n","          (dropout2): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (dense_layer): Sequential(\n","    (0): Linear(in_features=1026, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=1, bias=True)\n","    (3): Sigmoid()\n","  )\n",")"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# Load the saved state into the model and optimizer\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","# Set the model to evaluation mode\n","model.eval()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T17:27:48.932314Z","iopub.status.busy":"2024-09-27T17:27:48.931671Z","iopub.status.idle":"2024-09-27T17:27:49.493439Z","shell.execute_reply":"2024-09-27T17:27:49.492597Z","shell.execute_reply.started":"2024-09-27T17:27:48.932274Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","import torch\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T18:40:57.656340Z","iopub.status.busy":"2024-09-27T18:40:57.655962Z","iopub.status.idle":"2024-09-27T18:40:57.667404Z","shell.execute_reply":"2024-09-27T18:40:57.666466Z","shell.execute_reply.started":"2024-09-27T18:40:57.656304Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(model, dataloader, device):\n","    model.eval()  # Set the model to evaluation mode\n","    all_labels = []\n","    all_predictions = []\n","    all_probabilities = []\n","\n","    with torch.no_grad():\n","        for images, input_ids, attention_mask, followers_count, following_count, labels in dataloader:\n","            images = images.to(device)\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            followers_count = followers_count.to(device)\n","            following_count = following_count.to(device)\n","            labels = labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(images, input_ids, attention_mask, followers_count, following_count)\n","            probabilities = outputs.squeeze().cpu().numpy()\n","            predictions = (outputs.squeeze() > 0.5).int().cpu().numpy()\n","            labels = labels.cpu().numpy()\n","\n","            # Collect all labels and predictions\n","            all_labels.extend(labels)\n","            all_predictions.extend(predictions)\n","            all_probabilities.extend(probabilities)\n","\n","    # Convert lists to numpy arrays\n","    all_labels = np.array(all_labels)\n","    all_predictions = np.array(all_predictions)\n","    all_probabilities = np.array(all_probabilities)\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(all_labels, all_predictions)\n","    precision = precision_score(all_labels, all_predictions)\n","    recall = recall_score(all_labels, all_predictions)\n","    f1 = f1_score(all_labels, all_predictions)\n","    roc_auc = roc_auc_score(all_labels, all_probabilities)\n","\n","    print(f\"Accuracy: {accuracy}\")\n","    print(f\"Precision: {precision}\")\n","    print(f\"Recall: {recall}\")\n","    print(f\"F1 Score: {f1}\")\n","    print(f\"AUC-ROC: {roc_auc}\")\n","\n","    return {\n","        \"accuracy\": accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1\": f1,\n","        \"roc_auc\": roc_auc\n","    }\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T18:40:59.647049Z","iopub.status.busy":"2024-09-27T18:40:59.646604Z","iopub.status.idle":"2024-09-27T18:40:59.669400Z","shell.execute_reply":"2024-09-27T18:40:59.668634Z","shell.execute_reply.started":"2024-09-27T18:40:59.647009Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","df1=pd.read_csv('/kaggle/input/testing/Test.csv')\n","\n","dataset1 = BotDetectionDataset(df1, tokenizer, transform=transform)\n","\n","dataloader = DataLoader(dataset1, batch_size=4, shuffle=True)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T18:41:00.129987Z","iopub.status.busy":"2024-09-27T18:41:00.129608Z","iopub.status.idle":"2024-09-27T18:41:22.893563Z","shell.execute_reply":"2024-09-27T18:41:22.892570Z","shell.execute_reply.started":"2024-09-27T18:41:00.129949Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9019607843137255\n","Precision: 0.75\n","Recall: 0.9230769230769231\n","F1 Score: 0.8275862068965517\n","AUC-ROC: 0.9251012145748988\n","Final Model Metrics:\n","{'accuracy': 0.9019607843137255, 'precision': 0.75, 'recall': 0.9230769230769231, 'f1': 0.8275862068965517, 'roc_auc': 0.9251012145748988}\n"]}],"source":["# Assume you've already loaded your model and data\n","\n","# Call the evaluation function\n","metrics = evaluate_model(model, dataloader, device)\n","\n","# Access individual metrics\n","print(\"Final Model Metrics:\")\n","print(metrics)\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T19:16:40.446815Z","iopub.status.busy":"2024-09-24T19:16:40.446407Z","iopub.status.idle":"2024-09-24T19:16:40.451328Z","shell.execute_reply":"2024-09-24T19:16:40.450429Z","shell.execute_reply.started":"2024-09-24T19:16:40.446761Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5755167,"sourceId":9465259,"sourceType":"datasetVersion"},{"datasetId":5761188,"sourceId":9473191,"sourceType":"datasetVersion"},{"datasetId":5761242,"sourceId":9473254,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
