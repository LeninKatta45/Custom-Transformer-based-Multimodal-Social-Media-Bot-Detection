{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9465259,"sourceType":"datasetVersion","datasetId":5755167},{"sourceId":9473191,"sourceType":"datasetVersion","datasetId":5761188},{"sourceId":9473254,"sourceType":"datasetVersion","datasetId":5761242}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('/kaggle/input/social/Social.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-27T16:36:59.666557Z","iopub.execute_input":"2024-09-27T16:36:59.666941Z","iopub.status.idle":"2024-09-27T16:37:00.066619Z","shell.execute_reply.started":"2024-09-27T16:36:59.666903Z","shell.execute_reply":"2024-09-27T16:37:00.065119Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom transformers import BertTokenizer, BertModel\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-09-27T16:37:22.603783Z","iopub.execute_input":"2024-09-27T16:37:22.604182Z","iopub.status.idle":"2024-09-27T16:37:26.800174Z","shell.execute_reply.started":"2024-09-27T16:37:22.604140Z","shell.execute_reply":"2024-09-27T16:37:26.799255Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Transformations for Image Preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])","metadata":{"execution":{"iopub.status.busy":"2024-09-27T16:37:26.801707Z","iopub.execute_input":"2024-09-27T16:37:26.802174Z","iopub.status.idle":"2024-09-27T16:37:26.806789Z","shell.execute_reply.started":"2024-09-27T16:37:26.802138Z","shell.execute_reply":"2024-09-27T16:37:26.805823Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Data Preparation\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-09-27T16:37:38.128389Z","iopub.execute_input":"2024-09-27T16:37:38.128796Z","iopub.status.idle":"2024-09-27T16:37:39.790380Z","shell.execute_reply.started":"2024-09-27T16:37:38.128757Z","shell.execute_reply":"2024-09-27T16:37:39.789375Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4089ae40efe04af5b58476c104999f89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38cd909e14c84bf0985ee58f5f2fe97f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60bb37bbe968485d97344b93aca19292"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae7a4c9d23964e8886257740b98f45f7"}},"metadata":{}}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T16:37:33.067147Z","iopub.execute_input":"2024-09-27T16:37:33.067568Z","iopub.status.idle":"2024-09-27T16:37:33.072068Z","shell.execute_reply.started":"2024-09-27T16:37:33.067531Z","shell.execute_reply":"2024-09-27T16:37:33.071163Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom transformers import BertTokenizer, BertModel\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\n\n# Autoencoder for Image Feature Extraction\nclass Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(512, 256, kernel_size=3, stride=2, padding=1),  # Input from VGG16\n            nn.ReLU(),\n            nn.Conv2d(256, 128, kernel_size=3, stride=2, padding=1),  # Downsampling\n            nn.ReLU(),\n            nn.Conv2d(128, 64, kernel_size=3, stride=2, padding=1),  # Further downsampling\n            nn.ReLU(),\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(64, 128, kernel_size=3, stride=2),  # Upsampling\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 256, kernel_size=3, stride=2),  # Upsampling\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 3, kernel_size=3, stride=2, padding=1),  # Final output\n        )\n\n    def forward(self, x):\n        latent = self.encoder(x)\n        reconstructed = self.decoder(latent)\n        return latent, reconstructed\n\n# Image Encoder that uses VGG16 and Autoencoder\nclass ImageEncoder(nn.Module):\n    def __init__(self):\n        super(ImageEncoder, self).__init__()\n        vgg16 = models.vgg16(pretrained=True)\n        self.vgg16_features = vgg16.features\n        self.autoencoder = Autoencoder()\n    \n    def forward(self, x):\n        with torch.no_grad():\n            x = self.vgg16_features(x)  # Get feature maps from VGG16\n        latent, _ = self.autoencoder(x)  # Pass through autoencoder\n        return latent.view(latent.size(0), -1)  # Flatten for further processing\n\n# Text Encoder (e.g., BERT for Tweets)\nclass TextEncoder(nn.Module):\n    def __init__(self):\n        super(TextEncoder, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        return outputs.pooler_output  # CLS token embedding\n\n# Transformer Encoder for Combined Features\nclass TransformerEncoder(nn.Module):\n    def __init__(self, input_size, num_heads=4, ff_size=512, num_layers=2):\n        super(TransformerEncoder, self).__init__() \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=input_size, nhead=num_heads, dim_feedforward=ff_size\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n    def forward(self, x):\n        x = self.transformer_encoder(x)\n        return x\n\n# Bot Classification Model\nclass BotDetectionModel(nn.Module):\n    def __init__(self):\n        super(BotDetectionModel, self).__init__()\n        self.image_encoder = ImageEncoder()\n        self.text_encoder = TextEncoder()\n        self.transformer_encoder = TransformerEncoder(input_size=256 * 7 * 7 + 768)  # Image features + Text embeddings\n        self.dense_layer = nn.Sequential(\n            nn.Linear(256 * 7 * 7 + 768 + 2, 128),  # Adding followers and following counts\n            nn.ReLU(),\n            nn.Linear(128, 1),  # Binary classification (Bot or Not)\n            nn.Sigmoid()\n        )\n    \n    def forward(self, images, input_ids, attention_mask, followers_count, following_count):\n        # Process image inputs through VGG16 + Autoencoder\n        image_features = self.image_encoder(images)\n        \n        # Process text inputs through BERT\n        text_embeddings = self.text_encoder(input_ids, attention_mask)\n        \n        # Concatenate image and text embeddings\n        combined_features = torch.cat((image_features, text_embeddings), dim=1)\n        \n        # Forward through Transformer Encoder\n        transformer_output = self.transformer_encoder(combined_features.unsqueeze(1))  # Add sequence dimension\n        \n        # Add follower and following counts\n        combined_with_counts = torch.cat((transformer_output.squeeze(1), followers_count, following_count), dim=1)\n        \n        # Dense layer for final classification\n        output = self.dense_layer(combined_with_counts)\n        return output\n\n# Custom Dataset Class for Bot Detection\nclass BotDetectionDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, transform=None):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.transform = transform\n    \n    def load_image_from_url(self, url):\n        try:\n            response = requests.get(url)\n            img = Image.open(BytesIO(response.content)).convert('RGB')\n            if self.transform:\n                img = self.transform(img)\n            return img\n        except:\n            return torch.zeros(3, 224, 224)  # Handle invalid or missing images\n    \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n\n        # Load and preprocess images\n        profile_image = self.load_image_from_url(row['profile_image_url'])\n        banner_image = self.load_image_from_url(row['profile_banner_url'])\n        post_images = torch.stack([self.load_image_from_url(url) for url in eval(row['posts_url'])], dim=0).mean(0)  # Average across post images\n        \n        images = torch.stack([profile_image, banner_image, post_images], dim=0).mean(0)  # Average profile, banner, and post images\n        \n        # Tokenize the tweet text\n        tweet_text = row['Tweet']\n        encoding = self.tokenizer(tweet_text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n        input_ids = encoding['input_ids'].squeeze(0)\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        \n        # Followers and following counts\n        followers_count = torch.tensor([row['followers_count']], dtype=torch.float32)\n        following_count = torch.tensor([row['friends_count']], dtype=torch.float32)\n\n        # Label (0: Human, 1: Bot)\n        label = torch.tensor(1 if row['result'] == 'bot' else 0, dtype=torch.float32)\n        \n        return images, input_ids, attention_mask, followers_count, following_count, label","metadata":{"execution":{"iopub.status.busy":"2024-09-27T16:42:02.029408Z","iopub.execute_input":"2024-09-27T16:42:02.030238Z","iopub.status.idle":"2024-09-27T16:42:02.059797Z","shell.execute_reply.started":"2024-09-27T16:42:02.030196Z","shell.execute_reply":"2024-09-27T16:42:02.058887Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Transformations for Image Preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])","metadata":{"execution":{"iopub.status.busy":"2024-09-27T16:42:07.962242Z","iopub.execute_input":"2024-09-27T16:42:07.962627Z","iopub.status.idle":"2024-09-27T16:42:07.967383Z","shell.execute_reply.started":"2024-09-27T16:42:07.962592Z","shell.execute_reply":"2024-09-27T16:42:07.966394Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Assume 'df' is the pandas DataFrame containing your dataset\ndf = pd.read_csv('/kaggle/input/social/Social.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-27T16:42:11.058402Z","iopub.execute_input":"2024-09-27T16:42:11.058801Z","iopub.status.idle":"2024-09-27T16:42:11.075326Z","shell.execute_reply.started":"2024-09-27T16:42:11.058761Z","shell.execute_reply":"2024-09-27T16:42:11.074103Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom transformers import BertTokenizer, BertModel\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\n\n# VGG16 with Autoencoder for Image Feature Extraction\nclass ImageEncoder(nn.Module):\n    def __init__(self):\n        super(ImageEncoder, self).__init__()\n        vgg16 = models.vgg16(pretrained=True)\n        self.vgg16_features = vgg16.features\n        \n        # Encoder part of Autoencoder\n        self.encoder = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 1024),  # Assuming VGG output size (512, 7, 7)\n            nn.ReLU(),\n            nn.Linear(1024, 256)  # Latent space representation\n        )\n        \n        # Decoder part of Autoencoder\n        self.decoder = nn.Sequential(\n            nn.Linear(256, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512 * 7 * 7),\n            nn.Sigmoid()  # Reconstruct the input (if needed during training)\n        )\n    \n    def forward(self, x):\n        # Extract features from VGG16\n        with torch.no_grad():\n            x = self.vgg16_features(x)\n        x = torch.flatten(x, start_dim=1)\n        \n        # Encoder to get latent representation\n        latent_rep = self.encoder(x)\n        \n        # Decoder for training (optional, only needed if you want to reconstruct input during training)\n        reconstructed = self.decoder(latent_rep)\n        \n        return latent_rep, reconstructed\n\n# Text Encoder (e.g., BERT for Tweets)\nclass TextEncoder(nn.Module):\n    def __init__(self):\n        super(TextEncoder, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        return outputs.pooler_output  # CLS token embedding\n\n# Transformer Encoder for Combined Features\nclass TransformerEncoder(nn.Module):\n    def __init__(self, input_size, num_heads=4, ff_size=512, num_layers=2):\n        super(TransformerEncoder, self).__init__() \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=input_size, nhead=num_heads, dim_feedforward=ff_size\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n    def forward(self, x):\n        x = self.transformer_encoder(x)\n        return x\n\n# Bot Classification Model\nclass BotDetectionModel(nn.Module):\n    def __init__(self):\n        super(BotDetectionModel, self).__init__()\n        self.image_encoder = ImageEncoder()\n        self.text_encoder = TextEncoder()\n        self.transformer_encoder = TransformerEncoder(input_size=256 + 768)  # Image features (256) + Text embeddings (768)\n        self.dense_layer = nn.Sequential(\n            nn.Linear(256 + 768 + 2, 128),  # Adding followers and following counts\n            nn.ReLU(),\n            nn.Linear(128, 1),  # Binary classification (Bot or Not)\n            nn.Sigmoid()\n        )\n    \n    def forward(self, images, input_ids, attention_mask, followers_count, following_count):\n        # Process image inputs through VGG16 + Autoencoder (latent representation only)\n        image_features, _ = self.image_encoder(images)\n        \n        # Process text inputs through BERT\n        text_embeddings = self.text_encoder(input_ids, attention_mask)\n        \n        # Concatenate image and text embeddings\n        combined_features = torch.cat((image_features, text_embeddings), dim=1)\n        \n        # Forward through Transformer Encoder\n        transformer_output = self.transformer_encoder(combined_features.unsqueeze(1))  # Add sequence dimension\n        \n        # Add follower and following counts\n        combined_with_counts = torch.cat((transformer_output.squeeze(1), followers_count, following_count), dim=1)\n        \n        # Dense layer for final classification\n        output = self.dense_layer(combined_with_counts)\n        return output\n\n# Custom Dataset Class for Bot Detection\nclass BotDetectionDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, transform=None):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.transform = transform\n    \n    def load_image_from_url(self, url):\n        try:\n            response = requests.get(url)\n            img = Image.open(BytesIO(response.content)).convert('RGB')\n            if self.transform:\n                img = self.transform(img)\n            return img\n        except:\n            return torch.zeros(3, 224, 224)  # Handle invalid or missing images\n    \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n\n        # Load and preprocess images\n        profile_image = self.load_image_from_url(row['profile_image_url'])\n        banner_image = self.load_image_from_url(row['profile_banner_url'])\n        post_images = torch.stack([self.load_image_from_url(url) for url in eval(row['posts_url'])], dim=0).mean(0)  # Average across post images\n        \n        images = torch.stack([profile_image, banner_image, post_images], dim=0).mean(0)  # Average profile, banner, and post images\n        \n        # Tokenize the tweet text\n        tweet_text = row['Tweet']\n        encoding = self.tokenizer(tweet_text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n        input_ids = encoding['input_ids'].squeeze(0)\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        \n        # Followers and following counts\n        followers_count = torch.tensor([row['followers_count']], dtype=torch.float32)\n        following_count = torch.tensor([row['friends_count']], dtype=torch.float32)\n\n        # Label (0: Human, 1: Bot)\n        label = torch.tensor(1 if row['result'] == 'bot' else 0, dtype=torch.float32)\n        \n        return images, input_ids, attention_mask, followers_count, following_count, label\n\n# Transformations for Image Preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Data Preparation\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Assume 'df' is the pandas DataFrame contai\n# Creating Dataset and DataLoader\ndataset = BotDetectionDataset(df, tokenizer, transform=transform)\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n\n# Training the Model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BotDetectionModel().to(device)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Loop\nfor epoch in range(9):\n    model.train()\n    running_loss = 0.0\n    for images, input_ids, attention_mask, followers_count, following_count, labels in dataloader:\n        images = images.to(device)\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        followers_count = followers_count.to(device)\n        following_count = following_count.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(images, input_ids, attention_mask, followers_count, following_count)\n        loss = criterion(outputs.squeeze(), labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    print(f\"Epoch [{epoch+1}/10], Loss: {running_loss/len(dataloader)}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:03:52.925223Z","iopub.execute_input":"2024-09-27T18:03:52.925651Z","iopub.status.idle":"2024-09-27T18:30:42.537882Z","shell.execute_reply.started":"2024-09-27T18:03:52.925610Z","shell.execute_reply":"2024-09-27T18:30:42.536832Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 1.0211989841892481\nEpoch [2/10], Loss: 0.8612674545519841\nEpoch [3/10], Loss: 1.3297130643904949\nEpoch [4/10], Loss: 1.158986344436033\nEpoch [5/10], Loss: 0.5306251735530608\nEpoch [6/10], Loss: 0.4608929750000876\nEpoch [7/10], Loss: 0.3460945374338153\nEpoch [8/10], Loss: 1.06889152570109\nEpoch [9/10], Loss: 0.9070639966773585\n","output_type":"stream"}]},{"cell_type":"code","source":"# Inference on Specific Account\nmodel.eval()\nwith torch.no_grad():\n    # Sample account info (as you would provide during inference)\n    sample_row = df.iloc[0]\n    images, input_ids, attention_mask, followers_count, following_count, _ = dataset[0]\n    \n    # Add batch dimension and move to device\n    images = images.unsqueeze(0).to(device)\n    input_ids = input_ids.unsqueeze(0).to(device)\n    attention_mask = attention_mask.unsqueeze(0).to(device)\n    followers_count = followers_count.unsqueeze(0).to(device)\n    following_count = following_count.unsqueeze(0).to(device)\n    \n    # Model prediction\n    prediction = model(images, input_ids, attention_mask, followers_count, following_count)\n    print(f\"Bot Probability: {prediction.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:40:48.525059Z","iopub.execute_input":"2024-09-27T18:40:48.526038Z","iopub.status.idle":"2024-09-27T18:40:48.972820Z","shell.execute_reply.started":"2024-09-27T18:40:48.525998Z","shell.execute_reply":"2024-09-27T18:40:48.971808Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Bot Probability: 0.0064884936437010765\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:01:15.650550Z","iopub.execute_input":"2024-09-27T18:01:15.651302Z","iopub.status.idle":"2024-09-27T18:01:15.656037Z","shell.execute_reply.started":"2024-09-27T18:01:15.651261Z","shell.execute_reply":"2024-09-27T18:01:15.655146Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"3.983754140790552e-05\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    # Sample account info (as you would provide during inference)\n    sample_row = df.iloc[500]\n    print(sample_row)\n    images, input_ids, attention_mask, followers_count, following_count, _ = dataset[500]\n    \n    # Add batch dimension and move to device\n    images = images.unsqueeze(0).to(device)\n    input_ids = input_ids.unsqueeze(0).to(device)\n    attention_mask = attention_mask.unsqueeze(0).to(device)\n    followers_count = followers_count.unsqueeze(0).to(device)\n    following_count = following_count.unsqueeze(0).to(device)\n    \n    # Model prediction\n    prediction = model(images, input_ids, attention_mask, followers_count, following_count)\n    print(f\"Bot Probability: {prediction.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:01:33.810017Z","iopub.execute_input":"2024-09-27T18:01:33.810420Z","iopub.status.idle":"2024-09-27T18:01:34.037600Z","shell.execute_reply.started":"2024-09-27T18:01:33.810384Z","shell.execute_reply":"2024-09-27T18:01:34.036555Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"id                                                            468385317\ncreated_at_year                                                    2019\nscreen_name                                                MariaNicola2\nprofile_image_url     http://pbs.twimg.com/profile_images/7956854877...\nprofile_banner_url    https://pbs.twimg.com/profile_banners/46838531...\nfollowers_count                                                     287\nfriends_count                                                       165\nresult                                                              bot\nposts_url             ['https://pbs.twimg.com/media/Ecjk1g8WAAIhxrV?...\nTweet                 Only enough yet popular determine internationa...\nName: 500, dtype: object\nBot Probability: 0.9998499155044556\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n}, 'bot_detection_model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T06:26:24.962612Z","iopub.execute_input":"2024-09-24T06:26:24.963236Z","iopub.status.idle":"2024-09-24T06:26:28.263805Z","shell.execute_reply.started":"2024-09-24T06:26:24.963194Z","shell.execute_reply":"2024-09-24T06:26:28.262967Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Load the saved state into the model and optimizer\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n# Set the model to evaluation mode\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-09-24T19:19:57.121249Z","iopub.execute_input":"2024-09-24T19:19:57.122023Z","iopub.status.idle":"2024-09-24T19:19:57.150687Z","shell.execute_reply.started":"2024-09-24T19:19:57.121978Z","shell.execute_reply":"2024-09-24T19:19:57.149820Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"BotDetectionModel(\n  (image_encoder): ImageEncoder(\n    (vgg16_features): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU(inplace=True)\n      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU(inplace=True)\n      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (6): ReLU(inplace=True)\n      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (8): ReLU(inplace=True)\n      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (11): ReLU(inplace=True)\n      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (13): ReLU(inplace=True)\n      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (15): ReLU(inplace=True)\n      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (18): ReLU(inplace=True)\n      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (20): ReLU(inplace=True)\n      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (22): ReLU(inplace=True)\n      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (25): ReLU(inplace=True)\n      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (27): ReLU(inplace=True)\n      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (29): ReLU(inplace=True)\n      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (encoder): Sequential(\n      (0): Linear(in_features=25088, out_features=1024, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=1024, out_features=256, bias=True)\n    )\n    (decoder): Sequential(\n      (0): Linear(in_features=256, out_features=1024, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=1024, out_features=25088, bias=True)\n      (3): Sigmoid()\n    )\n  )\n  (text_encoder): TextEncoder(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSdpaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n  )\n  (transformer_encoder): TransformerEncoder(\n    (transformer_encoder): TransformerEncoder(\n      (layers): ModuleList(\n        (0-1): 2 x TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (linear1): Linear(in_features=1024, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=512, out_features=1024, bias=True)\n          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (dense_layer): Sequential(\n    (0): Linear(in_features=1026, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=1, bias=True)\n    (3): Sigmoid()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nimport torch\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T17:27:48.931671Z","iopub.execute_input":"2024-09-27T17:27:48.932314Z","iopub.status.idle":"2024-09-27T17:27:49.493439Z","shell.execute_reply.started":"2024-09-27T17:27:48.932274Z","shell.execute_reply":"2024-09-27T17:27:49.492597Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, dataloader, device):\n    model.eval()  # Set the model to evaluation mode\n    all_labels = []\n    all_predictions = []\n    all_probabilities = []\n\n    with torch.no_grad():\n        for images, input_ids, attention_mask, followers_count, following_count, labels in dataloader:\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            followers_count = followers_count.to(device)\n            following_count = following_count.to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(images, input_ids, attention_mask, followers_count, following_count)\n            probabilities = outputs.squeeze().cpu().numpy()\n            predictions = (outputs.squeeze() > 0.5).int().cpu().numpy()\n            labels = labels.cpu().numpy()\n\n            # Collect all labels and predictions\n            all_labels.extend(labels)\n            all_predictions.extend(predictions)\n            all_probabilities.extend(probabilities)\n\n    # Convert lists to numpy arrays\n    all_labels = np.array(all_labels)\n    all_predictions = np.array(all_predictions)\n    all_probabilities = np.array(all_probabilities)\n\n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_predictions)\n    precision = precision_score(all_labels, all_predictions)\n    recall = recall_score(all_labels, all_predictions)\n    f1 = f1_score(all_labels, all_predictions)\n    roc_auc = roc_auc_score(all_labels, all_probabilities)\n\n    print(f\"Accuracy: {accuracy}\")\n    print(f\"Precision: {precision}\")\n    print(f\"Recall: {recall}\")\n    print(f\"F1 Score: {f1}\")\n    print(f\"AUC-ROC: {roc_auc}\")\n\n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"roc_auc\": roc_auc\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:40:57.655962Z","iopub.execute_input":"2024-09-27T18:40:57.656340Z","iopub.status.idle":"2024-09-27T18:40:57.667404Z","shell.execute_reply.started":"2024-09-27T18:40:57.656304Z","shell.execute_reply":"2024-09-27T18:40:57.666466Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndf1=pd.read_csv('/kaggle/input/testing/Test.csv')\n\ndataset1 = BotDetectionDataset(df1, tokenizer, transform=transform)\n\ndataloader = DataLoader(dataset1, batch_size=4, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:40:59.646604Z","iopub.execute_input":"2024-09-27T18:40:59.647049Z","iopub.status.idle":"2024-09-27T18:40:59.669400Z","shell.execute_reply.started":"2024-09-27T18:40:59.647009Z","shell.execute_reply":"2024-09-27T18:40:59.668634Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Assume you've already loaded your model and data\n\n# Call the evaluation function\nmetrics = evaluate_model(model, dataloader, device)\n\n# Access individual metrics\nprint(\"Final Model Metrics:\")\nprint(metrics)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T18:41:00.129608Z","iopub.execute_input":"2024-09-27T18:41:00.129987Z","iopub.status.idle":"2024-09-27T18:41:22.893563Z","shell.execute_reply.started":"2024-09-27T18:41:00.129949Z","shell.execute_reply":"2024-09-27T18:41:22.892570Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Accuracy: 0.9019607843137255\nPrecision: 0.75\nRecall: 0.9230769230769231\nF1 Score: 0.8275862068965517\nAUC-ROC: 0.9251012145748988\nFinal Model Metrics:\n{'accuracy': 0.9019607843137255, 'precision': 0.75, 'recall': 0.9230769230769231, 'f1': 0.8275862068965517, 'roc_auc': 0.9251012145748988}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-24T19:16:40.446407Z","iopub.execute_input":"2024-09-24T19:16:40.446815Z","iopub.status.idle":"2024-09-24T19:16:40.451328Z","shell.execute_reply.started":"2024-09-24T19:16:40.446761Z","shell.execute_reply":"2024-09-24T19:16:40.450429Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}