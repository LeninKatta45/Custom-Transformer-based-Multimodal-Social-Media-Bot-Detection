{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7a24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Select database\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB (replace URI if needed)\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "db = client[\"admin\"]\n",
    "\n",
    "# Select collection\n",
    "collection=db[\"User\"]\n",
    "collection2=db[\"final\"]\n",
    "collection1=db[\"Tweet_1\"]\n",
    "collection3=db['output']\n",
    "collection4=db['split']\n",
    "import re\n",
    "pattern = r'https:\\/\\/\\S+|RT\\s*@\\w+:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b4bdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset update completed. Total missing documents in this batch: 2126\n"
     ]
    }
   ],
   "source": [
    "# Convert new data to DataFrame and append to existing dataset\n",
    "df_new = pd.DataFrame(data)\n",
    "df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "df_combined.to_csv(\"The_dataset.csv\", index=False)\n",
    "\n",
    "print(f\"Dataset update completed. Total missing documents in this batch: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5113175",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buffer = pd.DataFrame(data_buffer)\n",
    "df_buffer.to_csv(file_path, mode='a', index=False, header=not written_once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac19bc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Written 500 records to the file.\n",
      "✅ Written 1000 records to the file.\n",
      "✅ Written 1500 records to the file.\n",
      "✅ Written 2000 records to the file.\n",
      "✅ Written 2500 records to the file.\n",
      "✅ Written 3000 records to the file.\n",
      "✅ Written 3500 records to the file.\n",
      "✅ Written 4000 records to the file.\n",
      "Cursor closed.\n",
      "✅ Dataset update completed. Total missing documents in this batch: 859\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import CursorNotFound\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load existing dataset if it exists\n",
    "file_path = \"The_dataset.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    df_existing = pd.read_csv(file_path)\n",
    "else:\n",
    "    df_existing = pd.DataFrame()\n",
    "\n",
    "# Initialize variables\n",
    "data_buffer = []\n",
    "buffer_size = 500\n",
    "count = 0\n",
    "written_once = os.path.exists(file_path)\n",
    "\n",
    "# Define start and end index\n",
    "start_index = 0\n",
    "end_index = 4050\n",
    "\n",
    "# Preload collection3 and collection4 into dictionaries\n",
    "collection3_data = {doc[\"id\"]: doc for doc in collection3.find({}, {\"id\": 1, \"label\": 1})}\n",
    "collection4_data = {doc[\"id\"]: doc for doc in collection4.find({}, {\"id\": 1, \"split\": 1})}\n",
    "\n",
    "def process_document(doc1):\n",
    "    \"\"\"Processes a single document.\"\"\"\n",
    "    global count\n",
    "    id1 = doc1[\"author_id\"]\n",
    "    key = f\"u{id1}\"\n",
    "\n",
    "    # Check if the author exists in collection3 and is a bot\n",
    "    check = collection3_data.get(key)\n",
    "    if not check or check.get(\"label\") != \"human\":\n",
    "        count += 1\n",
    "        return None\n",
    "\n",
    "    # Fetch user profile data from collection\n",
    "    doc = collection.find_one({\"id\": key}, {\"profile_image_url\": 1, \"name\": 1, \"description\": 1, \"public_metrics\": 1, \"verified\": 1})\n",
    "    if not doc:\n",
    "        count += 1\n",
    "        return None\n",
    "\n",
    "    # Extract data\n",
    "    profile_image = doc.get(\"profile_image_url\", \"\")\n",
    "    text1 = doc.get('name', '')\n",
    "    text2 = doc.get('description', '')\n",
    "    text3 = doc1.get(\"text\", '')\n",
    "\n",
    "    finaltext = f\"The name is {text1} with description {text2} posts content is {text3}\"\n",
    "\n",
    "    public_metrics = doc.get(\"public_metrics\", {})\n",
    "    followers_count = public_metrics.get(\"followers_count\", 0)\n",
    "    following_count = public_metrics.get(\"following_count\", 0)\n",
    "    tweet_count = public_metrics.get(\"tweet_count\", 0)\n",
    "    listed_count = public_metrics.get(\"listed_count\", 0)\n",
    "    verified = 1 if doc.get(\"verified\", False) else 0\n",
    "\n",
    "    bot = 0\n",
    "    split = collection4_data.get(key, {}).get('split', '')\n",
    "\n",
    "    return {\n",
    "        \"id\": key,\n",
    "        \"profile_image_url\": profile_image,\n",
    "        \"final_text\": finaltext,\n",
    "        \"followers_count\": followers_count,\n",
    "        \"following_count\": following_count,\n",
    "        \"tweet_count\": tweet_count,\n",
    "        \"listed_count\": listed_count,\n",
    "        \"verified\": verified,\n",
    "        \"bot\": bot,\n",
    "        \"split\": split\n",
    "    }\n",
    "\n",
    "# Parallel Processing\n",
    "with client.start_session() as session:\n",
    "    try:\n",
    "        cursor = collection1.find(session=session, no_cursor_timeout=True).batch_size(1000)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = {}\n",
    "            \n",
    "            for index, doc1 in enumerate(cursor):\n",
    "                if index < start_index:\n",
    "                    continue  # Skip until start_index\n",
    "                \n",
    "                if index >= end_index:\n",
    "                    break  # Stop at end_index\n",
    "\n",
    "                future = executor.submit(process_document, doc1)\n",
    "                futures[future] = index\n",
    "\n",
    "                if len(futures) >= buffer_size:\n",
    "                    for future in futures:\n",
    "                        result = future.result()\n",
    "                        if result:\n",
    "                            data_buffer.append(result)\n",
    "                    \n",
    "                    # Write to CSV\n",
    "                    if data_buffer:\n",
    "                        df_buffer = pd.DataFrame(data_buffer)\n",
    "                        df_buffer.to_csv(file_path, mode='a', index=False, header=not written_once)\n",
    "                        written_once = True\n",
    "                        data_buffer.clear()\n",
    "                        print(f\"✅ Written {index + 1} records to the file.\")\n",
    "\n",
    "                    futures.clear()  # Clear futures dictionary\n",
    "\n",
    "    except CursorNotFound as e:\n",
    "        print(f\"CursorNotFound Error: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        print(\"Cursor closed.\")\n",
    "\n",
    "# Write remaining records\n",
    "if data_buffer:\n",
    "    df_buffer = pd.DataFrame(data_buffer)\n",
    "    df_buffer.to_csv(file_path, mode='a', index=False, header=not written_once)\n",
    "    print(\"Final buffer data written to file.\")\n",
    "\n",
    "print(f\"✅ Dataset update completed. Total missing documents in this batch: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91fc022d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial Number (0-based index): 219432\n",
      "Serial Number (1-based index): 219433\n"
     ]
    }
   ],
   "source": [
    "# The author_id to find\n",
    "target_author_id = 1101027055255224320\n",
    "\n",
    "# Find the document and get its _id\n",
    "doc = collection1.find_one({\"author_id\": target_author_id})\n",
    "if doc:\n",
    "    # Count how many documents come before it (sorted by _id)\n",
    "    index = collection1.count_documents({\"_id\": {\"$lt\": doc[\"_id\"]}})\n",
    "    print(f\"Serial Number (0-based index): {index}\")\n",
    "    print(f\"Serial Number (1-based index): {index + 1}\")\n",
    "else:\n",
    "    print(\"Document not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632b050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import CursorNotFound\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load existing dataset if it exists\n",
    "file_path = \"The_dataset.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    df_existing = pd.read_csv(file_path)\n",
    "else:\n",
    "    df_existing = pd.DataFrame()\n",
    "\n",
    "# Initialize variables\n",
    "data_buffer = []\n",
    "buffer_size = 500\n",
    "count = 0\n",
    "written_once = os.path.exists(file_path)\n",
    "\n",
    "# Define start and end index\n",
    "start_index = 0\n",
    "end_index = 4050\n",
    "\n",
    "# Preload collection3 and collection4 into dictionaries\n",
    "collection3_data = {doc[\"id\"]: doc for doc in collection3.find({}, {\"id\": 1, \"label\": 1})}\n",
    "collection4_data = {doc[\"id\"]: doc for doc in collection4.find({}, {\"id\": 1, \"split\": 1})}\n",
    "\n",
    "def process_document(doc1):\n",
    "    \"\"\"Processes a single document.\"\"\"\n",
    "    global count\n",
    "    id1 = doc1[\"author_id\"]\n",
    "    key = f\"u{id1}\"\n",
    "\n",
    "    # Check if the author exists in collection3 and is a bot\n",
    "    check = collection3_data.get(key)\n",
    "    if not check or check.get(\"label\") != \"human\":\n",
    "        count += 1\n",
    "        return None\n",
    "\n",
    "    # Fetch user profile data from collection\n",
    "    doc = collection.find_one({\"id\": key}, {\"profile_image_url\": 1, \"name\": 1, \"description\": 1, \"public_metrics\": 1, \"verified\": 1})\n",
    "    if not doc:\n",
    "        count += 1\n",
    "        return None\n",
    "\n",
    "    # Extract data\n",
    "    profile_image = doc.get(\"profile_image_url\", \"\")\n",
    "    text1 = doc.get('name', '')\n",
    "    text2 = doc.get('description', '')\n",
    "    text3 = doc1.get(\"text\", '')\n",
    "\n",
    "    finaltext = f\"The name is {text1} with description {text2} posts content is {text3}\"\n",
    "\n",
    "    public_metrics = doc.get(\"public_metrics\", {})\n",
    "    followers_count = public_metrics.get(\"followers_count\", 0)\n",
    "    following_count = public_metrics.get(\"following_count\", 0)\n",
    "    tweet_count = public_metrics.get(\"tweet_count\", 0)\n",
    "    listed_count = public_metrics.get(\"listed_count\", 0)\n",
    "    verified = 1 if doc.get(\"verified\", False) else 0\n",
    "\n",
    "    bot = 0\n",
    "    split = collection4_data.get(key, {}).get('split', '')\n",
    "\n",
    "    return {\n",
    "        \"id\": key,\n",
    "        \"profile_image_url\": profile_image,\n",
    "        \"final_text\": finaltext,\n",
    "        \"followers_count\": followers_count,\n",
    "        \"following_count\": following_count,\n",
    "        \"tweet_count\": tweet_count,\n",
    "        \"listed_count\": listed_count,\n",
    "        \"verified\": verified,\n",
    "        \"bot\": bot,\n",
    "        \"split\": split\n",
    "    }\n",
    "\n",
    "# Parallel Processing\n",
    "with client.start_session() as session:\n",
    "    try:\n",
    "        cursor = collection2.find(session=session, no_cursor_timeout=True).batch_size(1000)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = {}\n",
    "            \n",
    "            for index, doc1 in enumerate(cursor):\n",
    "                if index < start_index:\n",
    "                    continue  # Skip until start_index\n",
    "                \n",
    "                if index >= end_index:\n",
    "                    break  # Stop at end_index\n",
    "\n",
    "                future = executor.submit(process_document, doc1)\n",
    "                futures[future] = index\n",
    "\n",
    "                if len(futures) >= buffer_size:\n",
    "                    for future in futures:\n",
    "                        result = future.result()\n",
    "                        if result:\n",
    "                            data_buffer.append(result)\n",
    "                    \n",
    "                    # Write to CSV\n",
    "                    if data_buffer:\n",
    "                        df_buffer = pd.DataFrame(data_buffer)\n",
    "                        df_buffer.to_csv(file_path, mode='a', index=False, header=not written_once)\n",
    "                        written_once = True\n",
    "                        data_buffer.clear()\n",
    "                        print(f\"✅ Written {index + 1} records to the file.\")\n",
    "\n",
    "                    futures.clear()  # Clear futures dictionary\n",
    "\n",
    "    except CursorNotFound as e:\n",
    "        print(f\"CursorNotFound Error: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        print(\"Cursor closed.\")\n",
    "\n",
    "# Write remaining records\n",
    "if data_buffer:\n",
    "    df_buffer = pd.DataFrame(data_buffer)\n",
    "    df_buffer.to_csv(file_path, mode='a', index=False, header=not written_once)\n",
    "    print(\"Final buffer data written to file.\")\n",
    "\n",
    "print(f\"✅ Dataset update completed. Total missing documents in this batch: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d919f491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ collection2 is empty! No documents found.\n",
      "🔹 collection2 contains 0 documents.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "\n",
    "# 🔹 Connect to MongoDB\n",
    "\n",
    "# 🔹 Debugging Step 1: Check if collection2 has documents\n",
    "total_docs_collection2 = collection2.count_documents({})\n",
    "if total_docs_collection2 == 0:\n",
    "    print(\"⚠️ collection2 is empty! No documents found.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"🔹 collection2 contains {total_docs_collection2} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d00c5cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ collection2 is empty! No documents found.\n",
      "🔹 collection2 contains 0 documents.\n",
      "🔹 Fetched 0 IDs from collection2. Sample IDs: []\n",
      "⚠️ No documents found in collection with 'id' and 'created_at' fields.\n",
      "🔹 Sample document from collection: None\n",
      "🔹 Found 0 matching records in collection.\n",
      "🔹 Sample 'created_at' values: []\n",
      "🔹 Prepared 0 updates.\n",
      "⚠️ No documents updated. Check if IDs match and created_at exists.\n"
     ]
    }
   ],
   "source": [
    "# 🔹 Fetch all IDs from collection2\n",
    "ids_to_find = [doc[\"id\"] for doc in collection2.find({}, {\"id\": 1})]\n",
    "print(f\"🔹 Fetched {len(ids_to_find)} IDs from collection2. Sample IDs: {ids_to_find[:5]}\")\n",
    "\n",
    "# 🔹 Debugging Step 2: Check if IDs exist in collection\n",
    "sample_collection = collection.find_one({}, {\"id\": 1, \"created_at\": 1})\n",
    "if not sample_collection:\n",
    "    print(\"⚠️ No documents found in collection with 'id' and 'created_at' fields.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"🔹 Sample document from collection: {sample_collection}\")\n",
    "\n",
    "# 🔹 Fetch corresponding created_at values from collection\n",
    "collection_data = {\n",
    "    doc[\"id\"]: doc.get(\"created_at\", \"\") for doc in collection.find(\n",
    "        {\"id\": {\"$in\": ids_to_find}}, {\"id\": 1, \"created_at\": 1}\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"🔹 Found {len(collection_data)} matching records in collection.\")\n",
    "print(f\"🔹 Sample 'created_at' values: {list(collection_data.values())[:5]}\")\n",
    "\n",
    "# 🔹 Prepare bulk update operations\n",
    "bulk_updates = []\n",
    "for doc in collection2.find({}, {\"id\": 1}):\n",
    "    user_id = doc[\"id\"]\n",
    "    created_at = collection_data.get(user_id, \"\")\n",
    "\n",
    "    if created_at:  # Only process if created_at is found\n",
    "        match = re.search(r\"(\\d{4})\", created_at)\n",
    "        if match:\n",
    "            account_age = 2025 - int(match.group(1))  # Use current year\n",
    "            bulk_updates.append(UpdateOne({\"id\": user_id}, {\"$set\": {\"account_age\": account_age}}))\n",
    "\n",
    "print(f\"🔹 Prepared {len(bulk_updates)} updates.\")\n",
    "\n",
    "# 🔹 Debugging Step 3: Check if updates will be applied\n",
    "if bulk_updates:\n",
    "    result = collection2.bulk_write(bulk_updates)\n",
    "    print(f\"✅ Successfully updated {result.modified_count} documents with account_age.\")\n",
    "else:\n",
    "    print(\"⚠️ No documents updated. Check if IDs match and created_at exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b7c41d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
