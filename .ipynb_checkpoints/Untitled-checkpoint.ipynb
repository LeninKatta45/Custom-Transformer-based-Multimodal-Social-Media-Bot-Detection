{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7a24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Select database\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB (replace URI if needed)\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "db = client[\"admin\"]\n",
    "\n",
    "# Select collection\n",
    "collection=db[\"User\"]\n",
    "collection2=db[\"final\"]\n",
    "collection1=db[\"Tweet_1\"]\n",
    "collection3=db['output']\n",
    "collection4=db['split']\n",
    "import re\n",
    "pattern = r'https:\\/\\/\\S+|RT\\s*@\\w+:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b4bdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset update completed. Total missing documents in this batch: 2126\n"
     ]
    }
   ],
   "source": [
    "# Convert new data to DataFrame and append to existing dataset\n",
    "df_new = pd.DataFrame(data)\n",
    "df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "df_combined.to_csv(\"The_dataset.csv\", index=False)\n",
    "\n",
    "print(f\"Dataset update completed. Total missing documents in this batch: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5113175",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buffer = pd.DataFrame(data_buffer)\n",
    "df_buffer.to_csv(file_path, mode='a', index=False, header=not written_once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac19bc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Written 500 records to the file.\n",
      "‚úÖ Written 1000 records to the file.\n",
      "‚úÖ Written 1500 records to the file.\n",
      "‚úÖ Written 2000 records to the file.\n",
      "‚úÖ Written 2500 records to the file.\n",
      "‚úÖ Written 3000 records to the file.\n",
      "‚úÖ Written 3500 records to the file.\n",
      "‚úÖ Written 4000 records to the file.\n",
      "Cursor closed.\n",
      "‚úÖ Dataset update completed. Total missing documents in this batch: 859\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import CursorNotFound\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load existing dataset if it exists\n",
    "file_path = \"The_dataset.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    df_existing = pd.read_csv(file_path)\n",
    "else:\n",
    "    df_existing = pd.DataFrame()\n",
    "\n",
    "# Initialize variables\n",
    "data_buffer = []\n",
    "buffer_size = 500\n",
    "count = 0\n",
    "written_once = os.path.exists(file_path)\n",
    "\n",
    "# Define start and end index\n",
    "start_index = 0\n",
    "end_index = 4050\n",
    "\n",
    "# Preload collection3 and collection4 into dictionaries\n",
    "collection3_data = {doc[\"id\"]: doc for doc in collection3.find({}, {\"id\": 1, \"label\": 1})}\n",
    "collection4_data = {doc[\"id\"]: doc for doc in collection4.find({}, {\"id\": 1, \"split\": 1})}\n",
    "\n",
    "def process_document(doc1):\n",
    "    \"\"\"Processes a single document.\"\"\"\n",
    "    global count\n",
    "    id1 = doc1[\"author_id\"]\n",
    "    key = f\"u{id1}\"\n",
    "\n",
    "    # Check if the author exists in collection3 and is a bot\n",
    "    check = collection3_data.get(key)\n",
    "    if not check or check.get(\"label\") != \"human\":\n",
    "        count += 1\n",
    "        return None\n",
    "\n",
    "    # Fetch user profile data from collection\n",
    "    doc = collection.find_one({\"id\": key}, {\"profile_image_url\": 1, \"name\": 1, \"description\": 1, \"public_metrics\": 1, \"verified\": 1})\n",
    "    if not doc:\n",
    "        count += 1\n",
    "        return None\n",
    "\n",
    "    # Extract data\n",
    "    profile_image = doc.get(\"profile_image_url\", \"\")\n",
    "    text1 = doc.get('name', '')\n",
    "    text2 = doc.get('description', '')\n",
    "    text3 = doc1.get(\"text\", '')\n",
    "\n",
    "    finaltext = f\"The name is {text1} with description {text2} posts content is {text3}\"\n",
    "\n",
    "    public_metrics = doc.get(\"public_metrics\", {})\n",
    "    followers_count = public_metrics.get(\"followers_count\", 0)\n",
    "    following_count = public_metrics.get(\"following_count\", 0)\n",
    "    tweet_count = public_metrics.get(\"tweet_count\", 0)\n",
    "    listed_count = public_metrics.get(\"listed_count\", 0)\n",
    "    verified = 1 if doc.get(\"verified\", False) else 0\n",
    "\n",
    "    bot = 0\n",
    "    split = collection4_data.get(key, {}).get('split', '')\n",
    "\n",
    "    return {\n",
    "        \"id\": key,\n",
    "        \"profile_image_url\": profile_image,\n",
    "        \"final_text\": finaltext,\n",
    "        \"followers_count\": followers_count,\n",
    "        \"following_count\": following_count,\n",
    "        \"tweet_count\": tweet_count,\n",
    "        \"listed_count\": listed_count,\n",
    "        \"verified\": verified,\n",
    "        \"bot\": bot,\n",
    "        \"split\": split\n",
    "    }\n",
    "\n",
    "# Parallel Processing\n",
    "with client.start_session() as session:\n",
    "    try:\n",
    "        cursor = collection1.find(session=session, no_cursor_timeout=True).batch_size(1000)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = {}\n",
    "            \n",
    "            for index, doc1 in enumerate(cursor):\n",
    "                if index < start_index:\n",
    "                    continue  # Skip until start_index\n",
    "                \n",
    "                if index >= end_index:\n",
    "                    break  # Stop at end_index\n",
    "\n",
    "                future = executor.submit(process_document, doc1)\n",
    "                futures[future] = index\n",
    "\n",
    "                if len(futures) >= buffer_size:\n",
    "                    for future in futures:\n",
    "                        result = future.result()\n",
    "                        if result:\n",
    "                            data_buffer.append(result)\n",
    "                    \n",
    "                    # Write to CSV\n",
    "                    if data_buffer:\n",
    "                        df_buffer = pd.DataFrame(data_buffer)\n",
    "                        df_buffer.to_csv(file_path, mode='a', index=False, header=not written_once)\n",
    "                        written_once = True\n",
    "                        data_buffer.clear()\n",
    "                        print(f\"‚úÖ Written {index + 1} records to the file.\")\n",
    "\n",
    "                    futures.clear()  # Clear futures dictionary\n",
    "\n",
    "    except CursorNotFound as e:\n",
    "        print(f\"CursorNotFound Error: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        print(\"Cursor closed.\")\n",
    "\n",
    "# Write remaining records\n",
    "if data_buffer:\n",
    "    df_buffer = pd.DataFrame(data_buffer)\n",
    "    df_buffer.to_csv(file_path, mode='a', index=False, header=not written_once)\n",
    "    print(\"Final buffer data written to file.\")\n",
    "\n",
    "print(f\"‚úÖ Dataset update completed. Total missing documents in this batch: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91fc022d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial Number (0-based index): 219432\n",
      "Serial Number (1-based index): 219433\n"
     ]
    }
   ],
   "source": [
    "# The author_id to find\n",
    "target_author_id = 1101027055255224320\n",
    "\n",
    "# Find the document and get its _id\n",
    "doc = collection1.find_one({\"author_id\": target_author_id})\n",
    "if doc:\n",
    "    # Count how many documents come before it (sorted by _id)\n",
    "    index = collection1.count_documents({\"_id\": {\"$lt\": doc[\"_id\"]}})\n",
    "    print(f\"Serial Number (0-based index): {index}\")\n",
    "    print(f\"Serial Number (1-based index): {index + 1}\")\n",
    "else:\n",
    "    print(\"Document not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632b050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import CursorNotFound\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load existing dataset if it exists\n",
    "file_path = \"The_dataset.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    df_existing = pd.read_csv(file_path)\n",
    "else:\n",
    "    df_existing = pd.DataFrame()\n",
    "\n",
    "# Initialize variables\n",
    "data_buffer = []\n",
    "buffer_size = 500\n",
    "count = 0\n",
    "written_once = os.path.exists(file_path)\n",
    "\n",
    "# Define start and end index\n",
    "start_index = 0\n",
    "end_index = 4050\n",
    "\n",
    "# Preload collection3 and collection4 into dictionaries\n",
    "collection3_data = {doc[\"id\"]: doc for doc in collection3.find({}, {\"id\": 1, \"label\": 1})}\n",
    "collection4_data = {doc[\"id\"]: doc for doc in collection4.find({}, {\"id\": 1, \"split\": 1})}\n",
    "\n",
    "def process_document(doc1):\n",
    "    \"\"\"Processes a single document.\"\"\"\n",
    "    global count\n",
    "    id1 = doc1[\"author_id\"]\n",
    "    key = f\"u{id1}\"\n",
    "\n",
    "    # Check if the author exists in collection3 and is a bot\n",
    "    check = collection3_data.get(key)\n",
    "    if not check or check.get(\"label\") != \"human\":\n",
    "        count += 1\n",
    "        return None\n",
    "\n",
    "    # Fetch user profile data from collection\n",
    "    doc = collection.find_one({\"id\": key}, {\"profile_image_url\": 1, \"name\": 1, \"description\": 1, \"public_metrics\": 1, \"verified\": 1})\n",
    "    if not doc:\n",
    "        count += 1\n",
    "        return None\n",
    "\n",
    "    # Extract data\n",
    "    profile_image = doc.get(\"profile_image_url\", \"\")\n",
    "    text1 = doc.get('name', '')\n",
    "    text2 = doc.get('description', '')\n",
    "    text3 = doc1.get(\"text\", '')\n",
    "\n",
    "    finaltext = f\"The name is {text1} with description {text2} posts content is {text3}\"\n",
    "\n",
    "    public_metrics = doc.get(\"public_metrics\", {})\n",
    "    followers_count = public_metrics.get(\"followers_count\", 0)\n",
    "    following_count = public_metrics.get(\"following_count\", 0)\n",
    "    tweet_count = public_metrics.get(\"tweet_count\", 0)\n",
    "    listed_count = public_metrics.get(\"listed_count\", 0)\n",
    "    verified = 1 if doc.get(\"verified\", False) else 0\n",
    "\n",
    "    bot = 0\n",
    "    split = collection4_data.get(key, {}).get('split', '')\n",
    "\n",
    "    return {\n",
    "        \"id\": key,\n",
    "        \"profile_image_url\": profile_image,\n",
    "        \"final_text\": finaltext,\n",
    "        \"followers_count\": followers_count,\n",
    "        \"following_count\": following_count,\n",
    "        \"tweet_count\": tweet_count,\n",
    "        \"listed_count\": listed_count,\n",
    "        \"verified\": verified,\n",
    "        \"bot\": bot,\n",
    "        \"split\": split\n",
    "    }\n",
    "\n",
    "# Parallel Processing\n",
    "with client.start_session() as session:\n",
    "    try:\n",
    "        cursor = collection2.find(session=session, no_cursor_timeout=True).batch_size(1000)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = {}\n",
    "            \n",
    "            for index, doc1 in enumerate(cursor):\n",
    "                if index < start_index:\n",
    "                    continue  # Skip until start_index\n",
    "                \n",
    "                if index >= end_index:\n",
    "                    break  # Stop at end_index\n",
    "\n",
    "                future = executor.submit(process_document, doc1)\n",
    "                futures[future] = index\n",
    "\n",
    "                if len(futures) >= buffer_size:\n",
    "                    for future in futures:\n",
    "                        result = future.result()\n",
    "                        if result:\n",
    "                            data_buffer.append(result)\n",
    "                    \n",
    "                    # Write to CSV\n",
    "                    if data_buffer:\n",
    "                        df_buffer = pd.DataFrame(data_buffer)\n",
    "                        df_buffer.to_csv(file_path, mode='a', index=False, header=not written_once)\n",
    "                        written_once = True\n",
    "                        data_buffer.clear()\n",
    "                        print(f\"‚úÖ Written {index + 1} records to the file.\")\n",
    "\n",
    "                    futures.clear()  # Clear futures dictionary\n",
    "\n",
    "    except CursorNotFound as e:\n",
    "        print(f\"CursorNotFound Error: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        print(\"Cursor closed.\")\n",
    "\n",
    "# Write remaining records\n",
    "if data_buffer:\n",
    "    df_buffer = pd.DataFrame(data_buffer)\n",
    "    df_buffer.to_csv(file_path, mode='a', index=False, header=not written_once)\n",
    "    print(\"Final buffer data written to file.\")\n",
    "\n",
    "print(f\"‚úÖ Dataset update completed. Total missing documents in this batch: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d919f491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è collection2 is empty! No documents found.\n",
      "üîπ collection2 contains 0 documents.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "\n",
    "# üîπ Connect to MongoDB\n",
    "\n",
    "# üîπ Debugging Step 1: Check if collection2 has documents\n",
    "total_docs_collection2 = collection2.count_documents({})\n",
    "if total_docs_collection2 == 0:\n",
    "    print(\"‚ö†Ô∏è collection2 is empty! No documents found.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"üîπ collection2 contains {total_docs_collection2} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d00c5cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è collection2 is empty! No documents found.\n",
      "üîπ collection2 contains 0 documents.\n",
      "üîπ Fetched 0 IDs from collection2. Sample IDs: []\n",
      "‚ö†Ô∏è No documents found in collection with 'id' and 'created_at' fields.\n",
      "üîπ Sample document from collection: None\n",
      "üîπ Found 0 matching records in collection.\n",
      "üîπ Sample 'created_at' values: []\n",
      "üîπ Prepared 0 updates.\n",
      "‚ö†Ô∏è No documents updated. Check if IDs match and created_at exists.\n"
     ]
    }
   ],
   "source": [
    "# üîπ Fetch all IDs from collection2\n",
    "ids_to_find = [doc[\"id\"] for doc in collection2.find({}, {\"id\": 1})]\n",
    "print(f\"üîπ Fetched {len(ids_to_find)} IDs from collection2. Sample IDs: {ids_to_find[:5]}\")\n",
    "\n",
    "# üîπ Debugging Step 2: Check if IDs exist in collection\n",
    "sample_collection = collection.find_one({}, {\"id\": 1, \"created_at\": 1})\n",
    "if not sample_collection:\n",
    "    print(\"‚ö†Ô∏è No documents found in collection with 'id' and 'created_at' fields.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"üîπ Sample document from collection: {sample_collection}\")\n",
    "\n",
    "# üîπ Fetch corresponding created_at values from collection\n",
    "collection_data = {\n",
    "    doc[\"id\"]: doc.get(\"created_at\", \"\") for doc in collection.find(\n",
    "        {\"id\": {\"$in\": ids_to_find}}, {\"id\": 1, \"created_at\": 1}\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"üîπ Found {len(collection_data)} matching records in collection.\")\n",
    "print(f\"üîπ Sample 'created_at' values: {list(collection_data.values())[:5]}\")\n",
    "\n",
    "# üîπ Prepare bulk update operations\n",
    "bulk_updates = []\n",
    "for doc in collection2.find({}, {\"id\": 1}):\n",
    "    user_id = doc[\"id\"]\n",
    "    created_at = collection_data.get(user_id, \"\")\n",
    "\n",
    "    if created_at:  # Only process if created_at is found\n",
    "        match = re.search(r\"(\\d{4})\", created_at)\n",
    "        if match:\n",
    "            account_age = 2025 - int(match.group(1))  # Use current year\n",
    "            bulk_updates.append(UpdateOne({\"id\": user_id}, {\"$set\": {\"account_age\": account_age}}))\n",
    "\n",
    "print(f\"üîπ Prepared {len(bulk_updates)} updates.\")\n",
    "\n",
    "# üîπ Debugging Step 3: Check if updates will be applied\n",
    "if bulk_updates:\n",
    "    result = collection2.bulk_write(bulk_updates)\n",
    "    print(f\"‚úÖ Successfully updated {result.modified_count} documents with account_age.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No documents updated. Check if IDs match and created_at exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b7c41d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
