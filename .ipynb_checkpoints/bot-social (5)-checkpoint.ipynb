{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T09:18:48.290407Z",
     "iopub.status.busy": "2024-10-11T09:18:48.290004Z",
     "iopub.status.idle": "2024-10-11T09:18:48.306378Z",
     "shell.execute_reply": "2024-10-11T09:18:48.305331Z",
     "shell.execute_reply.started": "2024-10-11T09:18:48.290369Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('Social.csv')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# VGG16 with Autoencoder for Image Feature Extraction\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.vgg16_features = vgg16.features\n",
    "        \n",
    "        # Encoder part of Autoencoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 1024),  # Assuming VGG output size (512, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256)  # Latent space representation\n",
    "        )\n",
    "        \n",
    "        # Decoder part of Autoencoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512 * 7 * 7),\n",
    "            nn.Sigmoid()  # Reconstruct the input (if needed during training)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features from VGG16\n",
    "        with torch.no_grad():\n",
    "            x = self.vgg16_features(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        # Encoder to get latent representation\n",
    "        latent_rep = self.encoder(x)\n",
    "        \n",
    "        # Decoder for training (optional, only needed if you want to reconstruct input during training)\n",
    "        reconstructed = self.decoder(latent_rep)\n",
    "        \n",
    "        return latent_rep, reconstructed\n",
    "\n",
    "# Text Encoder (e.g., BERT for Tweets)\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output  # CLS token embedding\n",
    "\n",
    "# Transformer Encoder for Combined Features\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, num_heads=4, ff_size=512, num_layers=2):\n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_size, nhead=num_heads, dim_feedforward=ff_size\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "# Bot Classification Model\n",
    "class BotDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BotDetectionModel, self).__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.transformer_encoder = TransformerEncoder(input_size=256 + 768)  # Image features (256) + Text embeddings (768)\n",
    "        self.dense_layer = nn.Sequential(\n",
    "            nn.Linear(256 + 768 + 2, 128),  # Adding followers and following counts\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),  # Binary classification (Bot or Not)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, images, input_ids, attention_mask, followers_count, following_count):\n",
    "        # Process image inputs through VGG16 + Autoencoder (latent representation only)\n",
    "        image_features, _ = self.image_encoder(images)\n",
    "        \n",
    "        # Process text inputs through BERT\n",
    "        text_embeddings = self.text_encoder(input_ids, attention_mask)\n",
    "        \n",
    "        # Concatenate image and text embeddings\n",
    "        combined_features = torch.cat((image_features, text_embeddings), dim=1)\n",
    "        \n",
    "        # Forward through Transformer Encoder\n",
    "        transformer_output = self.transformer_encoder(combined_features.unsqueeze(1))  # Add sequence dimension\n",
    "        \n",
    "        # Add follower and following counts\n",
    "        combined_with_counts = torch.cat((transformer_output.squeeze(1), followers_count, following_count), dim=1)\n",
    "        \n",
    "        # Dense layer for final classification\n",
    "        output = self.dense_layer(combined_with_counts)\n",
    "        return output\n",
    "\n",
    "# Custom Dataset Class for Bot Detection\n",
    "class BotDetectionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "    \n",
    "    def load_image_from_url(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img\n",
    "        except:\n",
    "            return torch.zeros(3, 224, 224)  # Handle invalid or missing images\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Load and preprocess images\n",
    "        profile_image = self.load_image_from_url(row['profile_image_url'])\n",
    "        banner_image = self.load_image_from_url(row['profile_banner_url'])\n",
    "        post_images = torch.stack([self.load_image_from_url(url) for url in eval(row['posts_url'])], dim=0).mean(0)  # Average across post images\n",
    "        \n",
    "        images = torch.stack([profile_image, banner_image, post_images], dim=0).mean(0)  # Average profile, banner, and post images\n",
    "        \n",
    "        # Tokenize the tweet text\n",
    "        tweet_text = row['Tweet']\n",
    "        encoding = self.tokenizer(tweet_text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Followers and following counts\n",
    "        followers_count = torch.tensor([row['followers_count']], dtype=torch.float32)\n",
    "        following_count = torch.tensor([row['friends_count']], dtype=torch.float32)\n",
    "\n",
    "        # Label (0: Human, 1: Bot)\n",
    "        label = torch.tensor(1 if row['result'] == 'bot' else 0, dtype=torch.float32)\n",
    "        \n",
    "        return images, input_ids, attention_mask, followers_count, following_count, label\n",
    "\n",
    "# Transformations for Image Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Data Preparation\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Assume 'df' is the pandas DataFrame contai\n",
    "# Creating Dataset and DataLoader\n",
    "dataset = BotDetectionDataset(df, tokenizer, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Training the Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BotDetectionModel().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(9):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, input_ids, attention_mask, followers_count, following_count, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        followers_count = followers_count.to(device)\n",
    "        following_count = following_count.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images, input_ids, attention_mask, followers_count, following_count)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "# Inference on Specific Account\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Sample account info (as you would provide during inference)\n",
    "    sample_row = df.iloc[0]\n",
    "    images, input_ids, attention_mask, followers_count, following_count, _ = dataset[0]\n",
    "    \n",
    "    # Add batch dimension and move to device\n",
    "    images = images.unsqueeze(0).to(device)\n",
    "    input_ids = input_ids.unsqueeze(0).to(device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(device)\n",
    "    followers_count = followers_count.unsqueeze(0).to(device)\n",
    "    following_count = following_count.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Model prediction\n",
    "    prediction = model(images, input_ids, attention_mask, followers_count, following_count)\n",
    "    print(f\"Bot Probability: {prediction.item()}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Sample account info (as you would provide during inference)\n",
    "    sample_row = df.iloc[500]\n",
    "    print(sample_row)\n",
    "    images, input_ids, attention_mask, followers_count, following_count, _ = dataset[500]\n",
    "    \n",
    "    # Add batch dimension and move to device\n",
    "    images = images.unsqueeze(0).to(device)\n",
    "    input_ids = input_ids.unsqueeze(0).to(device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(device)\n",
    "    followers_count = followers_count.unsqueeze(0).to(device)\n",
    "    following_count = following_count.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Model prediction\n",
    "    prediction = model(images, input_ids, attention_mask, followers_count, following_count)\n",
    "    print(f\"Bot Probability: {prediction.item()}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'bot_detection_model.pth')\n",
    "\n",
    "\n",
    "checkpoint = torch.load('bot_detection_model (2).pth')\n",
    "\n",
    "# Load the saved state into the model and optimizer\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask, followers_count, following_count, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            followers_count = followers_count.to(device)\n",
    "            following_count = following_count.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, input_ids, attention_mask, followers_count, following_count)\n",
    "            probabilities = outputs.squeeze().cpu().numpy()\n",
    "            predictions = (outputs.squeeze() > 0.5).int().cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            # Collect all labels and predictions\n",
    "            all_labels.extend(labels)\n",
    "            all_predictions.extend(predictions)\n",
    "            all_probabilities.extend(probabilities)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probabilities)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"AUC-ROC: {roc_auc}\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc\n",
    "    }\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "df1=pd.read_csv('/kaggle/input/testing/Test.csv')\n",
    "\n",
    "dataset1 = BotDetectionDataset(df1, tokenizer, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset1, batch_size=4, shuffle=True)\n",
    "\n",
    "# Assume you've already loaded your model and data\n",
    "\n",
    "# Call the evaluation function\n",
    "metrics = evaluate_model(model, dataloader, device)\n",
    "\n",
    "# Access individual metrics\n",
    "print(\"Final Model Metrics:\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T19:16:40.446815Z",
     "iopub.status.busy": "2024-09-24T19:16:40.446407Z",
     "iopub.status.idle": "2024-09-24T19:16:40.451328Z",
     "shell.execute_reply": "2024-09-24T19:16:40.450429Z",
     "shell.execute_reply.started": "2024-09-24T19:16:40.446761Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# VGG16 Feature Extractor (No Autoencoder, since we process images separately)\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.vgg16_features = vgg16.features\n",
    "        self.projection = nn.Linear(512 * 7 * 7, 256)  # Project to a smaller size for attention\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.vgg16_features(x)  # Shape: (batch_size, 512, 7, 7)\n",
    "        x = torch.flatten(x, start_dim=1)  # Shape: (batch_size, 512*7*7)\n",
    "        x = self.projection(x)  # Shape: (batch_size, 256)\n",
    "        return x\n",
    "# Updated TextEncoder class\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=768, num_heads=8)\n",
    "    \n",
    "    def expand_attention_mask(self, attention_mask):\n",
    "        # Original attention_mask: (batch_size, seq_len)\n",
    "        # Convert to (seq_len, seq_len) by taking one batch's mask (assuming uniform padding across batch)\n",
    "        seq_len = attention_mask.size(1)\n",
    "        # Take the first mask and expand it to (seq_len, seq_len)\n",
    "        mask = attention_mask[0].unsqueeze(0)  # Shape: (1, seq_len)\n",
    "        mask = mask.expand(seq_len, seq_len)   # Shape: (seq_len, seq_len)\n",
    "        return mask.bool()  # Convert to boolean\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # BERT forward pass\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state  # Shape: (batch_size, seq_len, 768)\n",
    "        \n",
    "        # Transpose for MultiheadAttention: (seq_len, batch_size, embed_dim)\n",
    "        sequence_output = sequence_output.transpose(0, 1)  # Shape: (seq_len, batch_size, 768)\n",
    "        \n",
    "        # Prepare attention mask for self-attention (2D mask)\n",
    "        attn_mask = self.expand_attention_mask(attention_mask)  # Shape: (seq_len, seq_len)\n",
    "        # Invert for attention: True means ignore (padding), False means attend\n",
    "        attn_mask = ~attn_mask\n",
    "        \n",
    "        # Apply self-attention\n",
    "        attn_output, _ = self.self_attention(sequence_output, sequence_output, sequence_output, \n",
    "                                            attn_mask=attn_mask)\n",
    "        \n",
    "        # Transpose back and aggregate\n",
    "        attn_output = attn_output.transpose(0, 1)  # Shape: (batch_size, seq_len, 768)\n",
    "        return attn_output.mean(dim=1)  # Shape: (batch_size, 768)\n",
    "\n",
    "# Rest of the BotDetectionModel remains unchanged, included for context\n",
    "class BotDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BotDetectionModel, self).__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_self_attention = nn.MultiheadAttention(embed_dim=256, num_heads=4)\n",
    "        self.cross_attention = CrossAttention(embed_dim=256)\n",
    "        self.dense_layer = nn.Sequential(\n",
    "            nn.Linear(256 + 2, 128),  # Cross-attention output (256) + followers/following (2)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),  # Binary classification (Bot or Not)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, images, input_ids, attention_mask, followers_count, following_count):\n",
    "        batch_size, num_images = images.size(0), images.size(1)\n",
    "        \n",
    "        # Process each image individually\n",
    "        image_features = []\n",
    "        for i in range(num_images):\n",
    "            img = images[:, i]  # Shape: (batch_size, 3, 224, 224)\n",
    "            feat = self.image_encoder(img)  # Shape: (batch_size, 256)\n",
    "            image_features.append(feat)\n",
    "        \n",
    "        image_features = torch.stack(image_features, dim=1)  # Shape: (batch_size, num_images, 256)\n",
    "        image_features = image_features.transpose(0, 1)  # Shape: (num_images, batch_size, 256)\n",
    "        attn_image_features, _ = self.image_self_attention(image_features, image_features, image_features)\n",
    "        attn_image_features = attn_image_features.transpose(0, 1)  # Shape: (batch_size, num_images, 256)\n",
    "        \n",
    "        # Process text with self-attention\n",
    "        text_features = self.text_encoder(input_ids, attention_mask)  # Shape: (batch_size, 768)\n",
    "        \n",
    "        # Cross-attention between image and text features\n",
    "        combined_features = self.cross_attention(attn_image_features, text_features)  # Shape: (batch_size, 256)\n",
    "        \n",
    "        # Append followers and following counts\n",
    "        combined_with_counts = torch.cat((combined_features, followers_count, following_count), dim=1)\n",
    "        \n",
    "        # Dense layer for final classification\n",
    "        output = self.dense_layer(combined_with_counts)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# Cross-Attention between Image and Text Features\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=4)\n",
    "    \n",
    "    def forward(self, image_features, text_features):\n",
    "        # image_features: (batch_size, num_images, 256)\n",
    "        # text_features: (batch_size, 768)\n",
    "        text_features = text_features.unsqueeze(1)  # Shape: (batch_size, 1, 768)\n",
    "        image_features = image_features.transpose(0, 1)  # Shape: (num_images, batch_size, 256)\n",
    "        text_features = text_features.transpose(0, 1)  # Shape: (1, batch_size, 768)\n",
    "        \n",
    "        # Project text features to match image feature dimension\n",
    "        text_proj = nn.Linear(768, 256)(text_features)\n",
    "        \n",
    "        attn_output, _ = self.cross_attention(query=image_features, key=text_proj, value=text_proj)\n",
    "        return attn_output.transpose(0, 1).mean(dim=1)  # Shape: (batch_size, 256)\n",
    "\n",
    "\n",
    "# Custom Dataset Class (Updated to handle multiple images separately)\n",
    "class BotDetectionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "    \n",
    "    def load_image_from_url(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img\n",
    "        except:\n",
    "            return torch.zeros(3, 224, 224)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Load and preprocess images separately\n",
    "        profile_image = self.load_image_from_url(row['profile_image_url'])\n",
    "        banner_image = self.load_image_from_url(row['profile_banner_url'])\n",
    "        post_images = [self.load_image_from_url(url) for url in eval(row['posts_url'])]\n",
    "        if not post_images:  # If no post images, use a placeholder\n",
    "            post_images = [torch.zeros(3, 224, 224)]\n",
    "        post_images = torch.stack(post_images, dim=0).mean(0)  # Average post images\n",
    "        \n",
    "        # Stack all images (processed separately later)\n",
    "        images = torch.stack([profile_image, banner_image, post_images], dim=0)  # Shape: (3, 3, 224, 224)\n",
    "        \n",
    "        # Tokenize the tweet text\n",
    "        tweet_text = row['Tweet']\n",
    "        encoding = self.tokenizer(tweet_text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Followers and following counts\n",
    "        followers_count = torch.tensor([row['followers_count']], dtype=torch.float32)\n",
    "        following_count = torch.tensor([row['friends_count']], dtype=torch.float32)\n",
    "\n",
    "        # Label (0: Human, 1: Bot)\n",
    "        label = torch.tensor(1 if row['result'] == 'bot' else 0, dtype=torch.float32)\n",
    "        \n",
    "        return images, input_ids, attention_mask, followers_count, following_count, label\n",
    "\n",
    "# Transformations for Image Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Data Preparation\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "df = pd.read_csv('Social.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T19:16:40.446815Z",
     "iopub.status.busy": "2024-09-24T19:16:40.446407Z",
     "iopub.status.idle": "2024-09-24T19:16:40.451328Z",
     "shell.execute_reply": "2024-09-24T19:16:40.450429Z",
     "shell.execute_reply.started": "2024-09-24T19:16:40.446761Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = BotDetectionDataset(df, tokenizer, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Epoch is: 0\n",
      "The iteration started\n"
     ]
    }
   ],
   "source": [
    "# Training Loop (unchanged)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BotDetectionModel().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(9):\n",
    "    print(f\"The Epoch is: {epoch}\")\n",
    "    print(\"The iteration started\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, input_ids, attention_mask, followers_count, following_count, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        followers_count = followers_count.to(device)\n",
    "        following_count = following_count.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images, input_ids, attention_mask, followers_count, following_count)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/9], Loss: {running_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T19:16:40.446815Z",
     "iopub.status.busy": "2024-09-24T19:16:40.446407Z",
     "iopub.status.idle": "2024-09-24T19:16:40.451328Z",
     "shell.execute_reply": "2024-09-24T19:16:40.450429Z",
     "shell.execute_reply.started": "2024-09-24T19:16:40.446761Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inference and Evaluation code remains largely the same, just ensure input shapes match\n",
    "# Update evaluate_model function if needed to handle new model output\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask, followers_count, following_count, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            followers_count = followers_count.to(device)\n",
    "            following_count = following_count.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images, input_ids, attention_mask, followers_count, following_count)\n",
    "            probabilities = outputs.squeeze().cpu().numpy()\n",
    "            predictions = (outputs.squeeze() > 0.5).int().cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            all_labels.extend(labels)\n",
    "            all_predictions.extend(predictions)\n",
    "            all_probabilities.extend(probabilities)\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probabilities)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"AUC-ROC: {roc_auc}\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc\n",
    "    }\n",
    "\n",
    "# Test dataset evaluation\n",
    "df1 = pd.read_csv('/kaggle/input/testing/Test.csv')\n",
    "dataset1 = BotDetectionDataset(df1, tokenizer, transform=transform)\n",
    "dataloader1 = DataLoader(dataset1, batch_size=4, shuffle=True)\n",
    "metrics = evaluate_model(model, dataloader1, device)\n",
    "print(\"Final Model Metrics:\", metrics)\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'bot_detection_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5755167,
     "sourceId": 9465259,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5761188,
     "sourceId": 9473191,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5761242,
     "sourceId": 9473254,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
